# -*- coding: utf-8 -*-
"""Group09QBUS6850_pred_2024S2.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13gHk8Lgd7oC0XQWjyzCtz16SY6G-p6Zx

# Import data
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install torchtext==0.16.0
!python -m spacy download en_core_web_lg

import pickle
import json
import re
import shap
import torch
import spacy
import random
import numpy as np
import pandas as pd
import seaborn as sns
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from collections import Counter

import xgboost as xgb
from sklearn.decomposition import TruncatedSVD
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, log_loss


from gensim.models import Word2Vec
import torch.nn as nn
import torch.optim as optim
from torch import utils
from torchtext.data.utils import get_tokenizer
from torch.utils.data import DataLoader, Dataset
from torchtext.vocab import build_vocab_from_iterator
import collections
from torch.nn.utils.rnn import pad_sequence
import torch.nn.functional as F
from transformers import DistilBertTokenizer, DistilBertModel
from torch.utils.data import DataLoader, TensorDataset


seed_value = 3407
random.seed(seed_value)
np.random.seed(3407)
torch.manual_seed(3407)
torch.cuda.manual_seed(3407)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

checkpoint = torch.load("/content/drive/MyDrive/6850/Group09QBUS6850_best_2024S2.pt")

competition = pd.read_csv('/content/drive/MyDrive/6850/news-challenge.csv',sep='\t')

"""# 1.0 Load best model's structure"""

import torch
import torch.nn as nn
from torch.nn.utils.rnn import pad_sequence

class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_LstmLayer, dropout_rate, fc1_dim):
        super().__init__()

        self.hidden_dim = hidden_dim
        self.num_LstmLayer = num_LstmLayer

        # 1. Embedding layer
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        # 2. Layer Normalizaion after embedding
        self.embed_norm = nn.LayerNorm(embed_dim)
        # 3. LSTM layer
        self.lstm = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim, num_layers=num_LstmLayer, batch_first=True)
        # 4. Dropout layer (moved after LSTM)
        self.dropout = nn.Dropout(p=dropout_rate)
        # 5. First fully connected layer
        self.Linear1 = nn.Linear(hidden_dim, fc1_dim)
        # 6. Second fully connected layer
        self.Linear2 = nn.Linear(fc1_dim, 5)

        # For residual connection in the fully connected layers
        if hidden_dim != fc1_dim:
            self.residual_fc = nn.Linear(hidden_dim, fc1_dim)
        else:
            self.residual_fc = None

    def forward(self, x):
        x = pad_sequence(x, batch_first=True, padding_value=0)

        # Initialize hidden state and cell state
        h = torch.zeros((self.num_LstmLayer, x.size(0), self.hidden_dim)).to(x.device)
        c = torch.zeros((self.num_LstmLayer, x.size(0), self.hidden_dim)).to(x.device)

        # 1. Embedding layer
        out = self.embedding(x)
        # 2. Nrom after Embedding
        out = self.embed_norm(out)
        # 3. LSTM layer
        out, (hidden, cell) = self.lstm(out, (h, c))
        # 4. Layer Normalization and Dropout
        out, _ = torch.max(out, dim=1)
        out = self.dropout(out)

        # Residual connection starts here
        # 5. First fully connected layer with possible residual
        residual = out  # Save for the residual connection
        out = self.Linear1(out)
        out = torch.relu(out)

        if self.residual_fc is not None:
            residual = self.residual_fc(residual)  # Make dimensions match if needed
        out = out + residual  # Add the residual connection

        # 6. Second fully connected layer
        out = self.Linear2(out)

        return out

LSTM_best_params = checkpoint['best_params']
Best_model = LSTMClassifier(
    vocab_size=LSTM_best_params['vocab_size'],
    embed_dim=LSTM_best_params['embed_dim'],
    hidden_dim=LSTM_best_params['hidden_dim'],
    num_LstmLayer=LSTM_best_params['num_LstmLayer'],
    dropout_rate=LSTM_best_params['dropout_rate'],
    fc1_dim = LSTM_best_params['fc1_dim']
)

Best_model.load_state_dict(checkpoint['model_state_dict'])

"""# 2.0 Predict"""

vocabulary = checkpoint['vocabulary']

vocabulary.set_default_index(vocabulary["<unk>"])

competition = competition[['content']]

nlp = spacy.load('en_core_web_lg') #create an object
def tokenizer(text):
    text = re.sub(r'#(\S+)', 'xxhashtag ' + r'\1', text) # hashtag

    text = re.sub(r'\s{2,}', ' ', text)

    doc = nlp(text)

    tokens = []

    for token in doc:

        word = token.lemma_.lower()

        if not token.is_stop:

            if word == '!':
                tokens.append('!')

            elif (not token.is_punct) and word != 'Ô∏è':
                tokens.append(word)

    return tokens

competition.iloc[:,0] = competition.iloc[:,0].apply(tokenizer)

competition['content'] = competition['content'].apply(lambda x: ' '.join(x))

class TextDataset_competition(utils.data.Dataset):
    def __init__(self, myData):
        """
        myData: a dataframe object containing only X (input data).
        """
        super().__init__()
        self.data = myData

    def __len__(self):
        return len(self.data)
    def __getitem__(self, idx):
        return self.data.iloc[idx]

competition_torch = TextDataset_competition(competition['content'])

tokenizer = get_tokenizer('basic_english')

def doc_tokenizer(doc):
    return torch.tensor([vocabulary[token] for token in tokenizer(doc)], dtype=torch.long)

def collate_batch_competition(batch):

    text_list = []

    # loop through all samples in batch
    for idx in range(len(batch)):

        _text = batch[idx]
        tokens = doc_tokenizer( _text )
        text_list.append(tokens)

    # convert to torch tensor

    return text_list

batchSize = 16
competition_loader = utils.data.DataLoader(competition_torch, batch_size=batchSize, shuffle=False, collate_fn=collate_batch_competition)

def predict_competition(model, data_loader):
    model.eval()  # evaluate
    all_preds = []  # save the classes
    all_probs = []  # save the probability
    with torch.no_grad():
        for texts in data_loader:
            outputs = model(texts)  # forward propagation
            probabilities = F.softmax(outputs, dim=1)  # calculate probabilities through softmax
            predicted_classes = torch.argmax(probabilities, dim=1)  # get the class(the max probabilities)

            all_preds.extend(predicted_classes.numpy())  # save the class into all_preds
            all_probs.extend(probabilities.numpy())  # save the probability into all_probs

    return np.array(all_preds), np.array(all_probs)  # return category and probability

predictions, probabilities = predict_competition(Best_model, competition_loader)

predictions

category_to_label = {
    'business': 0,
    'entertainment': 1,
    'politic': 2,
    'sport': 3,
    'tech': 4
}
label_to_category = {v: k for k, v in category_to_label.items()}

ori_categories = [label_to_category.get(label) for label in predictions]

print(ori_categories)

predictions_df = pd.DataFrame({
    'ID': range(len(ori_categories)),  # Create an ID column, sorted in order
    'category': ori_categories  # the predicted class (category)
})

predictions_df.head(30)

output_path = '/content/drive/MyDrive/6850/Group09QBUS6850_2024S2.csv'
# save datafrme into csv file
predictions_df.to_csv(output_path, index=False)