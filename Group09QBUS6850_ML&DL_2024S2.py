# -*- coding: utf-8 -*-
"""Group09QBUS6850_ML&DL_2024S2.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_5SSGcfYgVWKecf0kdAznJJyQb5gpFay

#RUN Through CPU

# Import data
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install optuna
!pip install shap
!pip install optuna-integration[xgboost]
!pip install torchtext==0.16.0

!python -m spacy download en_core_web_lg

import warnings
warnings.filterwarnings('ignore')

import pickle
import json
import re
import shap
import torch
import spacy
import random
import numpy as np
import pandas as pd
import seaborn as sns
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from collections import Counter

import optuna
import xgboost as xgb
import optuna.visualization as vis
from sklearn.decomposition import TruncatedSVD
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, log_loss


from gensim.models import Word2Vec
import torch.nn as nn
import torch.optim as optim
from torch import utils
from torchtext.data.utils import get_tokenizer
from torch.utils.data import DataLoader, Dataset
from torchtext.vocab import build_vocab_from_iterator
import collections
from torch.nn.utils.rnn import pad_sequence
import torch.nn.functional as F
from transformers import DistilBertTokenizer, DistilBertModel
from torch.utils.data import DataLoader, TensorDataset


seed_value = 42
random.seed(seed_value)
np.random.seed(42)
torch.manual_seed(42)
torch.cuda.manual_seed(42)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

checkpoint = torch.load('/content/drive/MyDrive/6850/checkpoint.pth')

assert checkpoint.get('device') == 'cpu', "Checkpoint was saved on a CPU, but is being loaded elsewhere."

data = pd.read_csv('/content/drive/MyDrive/6850/news-dataset.csv',sep='\t')

"""# 1.0 Data preprocessing"""

data

data.describe()

data.info()

data.nunique()

dup_content = data[data.duplicated(subset='content', keep=False)]
dup_content.head(20)

content_clean = data.drop_duplicates(subset='content')
content_clean.describe()
## We found there remain some duplicate content(ie content with higher similarity but not identify as duplicate)

title_dup = content_clean[content_clean.duplicated(subset='title', keep=False)]
title_dup.head(20)

nlp = spacy.load('en_core_web_lg') #create an object

def tokenizer(text):
    text = re.sub(r'#(\S+)', 'xxhashtag ' + r'\1', text) # hashtag

    text = re.sub(r'\s{2,}', ' ', text)

    doc = nlp(text)

    tokens = []

    for token in doc:

        word = token.lemma_.lower()

        if not token.is_stop:

            if word == '!':
                tokens.append('!')

            elif (not token.is_punct) and word != '️':
                tokens.append(word)

    return tokens

corpus = content_clean.copy()
for i in range(len(corpus.columns)):
      corpus.iloc[:,i] = corpus.iloc[:,i].apply(tokenizer)

content_clean = corpus.copy()

corpus

"""# 2.0 EDA"""

content_clean['category'] = content_clean['category'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)

content_clean['category'].unique()

def generate_wordcloud(text, category):
    wordcloud = WordCloud(width=1000, height=500, background_color='white').generate(' '.join(text))
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(f"Word Cloud for {category}")
    plt.axis('off')
    plt.show()

categories = content_clean['category'].unique()

for category in categories:
    category_text = content_clean[content_clean['category'] == category]['content'].str.join(' ')
    generate_wordcloud(category_text, category)

overall_text = content_clean['content'].str.join(' ').tolist()
generate_wordcloud(overall_text, 'Overall')

content_clean.isnull().sum()

cate_count = content_clean['category'].value_counts()

print(cate_count)

# Caculate category percentage
category_counts = content_clean['category'].value_counts()

category_percentage = (category_counts / category_counts.sum())

category_percentage

plt.figure(figsize=(12, 10))
sns.barplot(x=category_counts.index, y=category_counts.values, palette='coolwarm')
plt.title('Distribution of News Categories')
plt.xlabel('Categories')
plt.ylabel('Article Amount')
plt.xticks
plt.show()

content_clean['title_length'] = content_clean['title'].str.len()
content_clean['content_length'] = content_clean['content'].str.len()

stats = content_clean[['title_length', 'content_length']].agg(['min', 'max', 'mean'])

print(stats)

"""# 3.0 Feature engineering

## 3.1 Word Embedding
"""

sentences = corpus['content'].tolist()

word2vec_model = Word2Vec(sentences, vector_size=300, window=9, min_count=2, workers=1, negative = 10, alpha = 0.025, min_alpha = 0.0001, sg =1,seed = 42)
# After identify remaining duplicate content, we choose to use cosine similarity based on the distence between contents first we use word embedding to convert token into words vector.

def sentence_to_vector(sentence, model):
    word_vectors = [model.wv[word] for word in sentence if word in model.wv]
    if len(word_vectors) > 0:
        return np.mean(word_vectors, axis=0)
    else:
        return np.zeros(model.vector_size)

corpus_vectors = np.array([sentence_to_vector(sentence, word2vec_model) for sentence in sentences])

print("Corpus shape: ", corpus_vectors.shape)
# We define a function to convert into vector, this may help to calculate the similarity then.

"""### **3.1.1 Cosine similarity**"""

similarity_matrix = cosine_similarity(corpus_vectors)

print("The minimum:", np.min(similarity_matrix))
print("The maximum:", np.max(similarity_matrix))
print("The mean:", np.mean(similarity_matrix))

threshold = 0.994
to_remove = set()

for i in range(len(similarity_matrix)):
    for j in range(i + 1, len(similarity_matrix)):
        if similarity_matrix[i][j] > threshold:
            to_remove.add(j)

to_remove_list = list(to_remove)

to_remove_array = np.array(to_remove_list)
to_remove_array.shape

corpus = corpus.drop(corpus.index[to_remove_array]) # we drop the content which has high similarity

corpus.info()

corpus.head(30)

"""## 3.2 Train test split"""

index_train, index_test = train_test_split(corpus.index,stratify=corpus['category'], train_size=0.7, random_state = 42)

train = corpus.loc[index_train, :].copy()
test = corpus.loc[index_test, :].copy()

index_train1, index_valid1 = train_test_split(train.index, stratify = train['category'], train_size=0.7, random_state=42)

sub_train = train.loc[index_train1, :].copy()
sub_valid = train.loc[index_valid1, :].copy()

y_train = sub_train['category'].copy()
y_valid = sub_valid['category'].copy()
y_test = test['category'].copy()

category_distribution = y_train.value_counts()
category_distribution

sentences_train = sub_train['content'].tolist()
sentences_valid = sub_valid['content'].tolist()

word2vec_train = Word2Vec(sentences=sentences_train, vector_size=300, window=9, min_count=2, workers=4, negative = 10, alpha = 0.025, min_alpha = 0.0001, sg =1,seed = 42)
vocab = set(word2vec_train.wv.index_to_key)
vocab.add("<UNK>")

processed_valid = []
for sentence in sentences_valid: # the words not seen in train we give the <unk>
    processed_sentence = [word if word in vocab else "<UNK>" for word in sentence]
    processed_valid.append(processed_sentence)

def sentence_to_vector(sentence, model):
    word_vectors = []
    for word in sentence:
        if word in model.wv:
            word_vectors.append(model.wv[word])
        else:
            word_vectors.append(np.zeros(model.vector_size))

    if word_vectors:
        return np.mean(word_vectors, axis=0)
    else:
        return np.zeros(model.vector_size)

trainvector_word2vec = np.array([sentence_to_vector(sentence, word2vec_train) for sentence in sentences_train])
validvector_word2vec = np.array([sentence_to_vector(sentence, word2vec_train) for sentence in processed_valid])


print("Train shape: ", trainvector_word2vec.shape)
print("Valid shape: ", validvector_word2vec.shape)

trainvector_word2vec = Normalizer().fit_transform(trainvector_word2vec) #Normalization (mean = 0, st dev = 1)
validvector_word2vec = Normalizer().transform(validvector_word2vec)

"""## 3.3 Encoding"""

sub_train['content'] = sub_train['content'].apply(lambda x: ' '.join(x))
sub_valid['content'] = sub_valid['content'].apply(lambda x: ' '.join(x))
test['content'] = test['content'].apply(lambda x: ' '.join(x))

"""### 3.3.1 Bag of Words"""

vectorizer = CountVectorizer()

x_bow_train = vectorizer.fit_transform(sub_train['content'])
x_bow_valid = vectorizer.transform(sub_valid['content'])

x_bow_train = Normalizer().fit_transform(x_bow_train) #Normalization (mean = 0, st dev = 1)
x_bow_valid = Normalizer().transform(x_bow_valid)

feature_names = vectorizer.get_feature_names_out()

bagofwords_train = pd.DataFrame(x_bow_train.toarray(), columns=feature_names)
bagofwords_valid = pd.DataFrame(x_bow_valid.toarray(), columns=feature_names)

print(bagofwords_train.shape)
print(bagofwords_valid.shape)

svd = TruncatedSVD(n_components=100)
x = np.arange(100)

x_bow_train = svd.fit_transform(x_bow_train) #Singular Value decomposition, i.e. dimension reduction
x_bow_valid = svd.transform(x_bow_valid)

"""### 3.3.2 N-gram(bigram)"""

vectorizer = CountVectorizer(ngram_range=(2, 2))

x_bigram_train = vectorizer.fit_transform(sub_train['content'])
x_bigram_valid = vectorizer.transform(sub_valid['content'])

x_bigram_train = Normalizer().fit_transform(x_bigram_train) #Normalization (mean = 0, st dev = 1)
x_bigram_valid = Normalizer().transform(x_bigram_valid)

feature_names = vectorizer.get_feature_names_out()

bigram_train = pd.DataFrame(x_bigram_train.toarray(), columns=feature_names)
bigram_valid = pd.DataFrame(x_bigram_valid.toarray(), columns=feature_names)

print(bigram_train.shape)
print(bigram_valid.shape)

svd = TruncatedSVD(n_components=100)
x = np.arange(100)

x_bigram_train = svd.fit_transform(x_bigram_train) #Singular Value decomposition, i.e. dimension reduction
x_bigram_valid = svd.transform(x_bigram_valid)

"""### 3.3.3 N-gram(trigram)"""

vectorizer = CountVectorizer(ngram_range=(3, 3))

x_trigram_train = vectorizer.fit_transform(sub_train['content'])
x_trigram_valid = vectorizer.transform(sub_valid['content'])

x_trigram_train = Normalizer().fit_transform(x_trigram_train) #Normalization (mean = 0, st dev = 1)
x_trigram_valid = Normalizer().transform(x_trigram_valid)

feature_names = vectorizer.get_feature_names_out()

trigram_train = pd.DataFrame(x_trigram_train.toarray(), columns=feature_names)
trigram_valid = pd.DataFrame(x_trigram_valid.toarray(), columns=feature_names)

print(trigram_train.shape)
print(trigram_valid.shape)

svd = TruncatedSVD(n_components=100)
x = np.arange(100)

x_trigram_train = svd.fit_transform(x_trigram_train) #Singular Value decomposition, i.e. dimension reduction
x_trigram_valid = svd.transform(x_trigram_valid)

"""### 3.3.4 TF-IDF"""

tfidf_vectorizer = TfidfVectorizer(norm='l2', smooth_idf=False)

x_tfidf_train = tfidf_vectorizer.fit_transform(sub_train['content'])
x_tfidf_valid = tfidf_vectorizer.transform(sub_valid['content'])

x_tfidf_train = Normalizer().fit_transform(x_tfidf_train) #Normalization (mean = 0, st dev = 1)
x_tfidf_valid = Normalizer().transform(x_tfidf_valid)

feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()

tfidf_train = pd.DataFrame(x_tfidf_train.toarray(), columns=feature_names_tfidf)
tfidf_valid = pd.DataFrame(x_tfidf_valid.toarray(), columns=feature_names_tfidf)

print(tfidf_train.shape)
print(tfidf_valid.shape)

svd = TruncatedSVD(n_components=100)
x = np.arange(100)

x_tfidf_train = svd.fit_transform(x_tfidf_train) #Singular Value decomposition, i.e. dimension reduction
x_tfidf_valid = svd.transform(x_tfidf_valid)

"""## 3.4 Label Encoding"""

y_train_cleaned = y_train.apply(lambda x: x[0].strip('[]'))
y_valid_cleaned = y_valid.apply(lambda x: x[0].strip('[]'))
y_test_cleaned = y_test.apply(lambda x: x[0].strip('[]'))

category_to_label = {
    'business': 0,
    'sport': 1,
    'politic': 2,
    'entertainment': 3,
    'tech': 4
}
y_train_encoded = y_train_cleaned.map(category_to_label)
y_valid_encoded = y_valid_cleaned.map(category_to_label)
y_test_encoded = y_test_cleaned.map(category_to_label)

print(y_train_encoded)
print(y_valid_encoded)

"""# 4.0 Model building (Machine learning)

## 4.1 Logistic

### 4.1.1 Word2Vec logit

#### 4.1.1.1 Word2Vec No penalty

>
"""

# multinomial is suitable for our task, 5 class, this model is based on word2vec process and with no penalty
logit_word2vec = LogisticRegression(penalty= None, multi_class='multinomial', solver='lbfgs', random_state=3407)
logit_word2vec.fit(trainvector_word2vec, y_train_encoded)

logit_pred_word2vec = logit_word2vec.predict(validvector_word2vec)
acc_logit_w2v = accuracy_score(y_valid_encoded, logit_pred_word2vec)
print("Accuracy_Score_w2v_logit", acc_logit_w2v)

"""#### 4.1.1.2 Word2Vec L1"""

alphas = np.logspace(-5, 5, 50, base=10)

# Create a logistic regression model with L2 regularization
# Cs are the inverse of regularization strengths
l1_word2vec = LogisticRegressionCV(Cs=1/alphas, cv=5, penalty='l1', solver='liblinear', multi_class='ovr', random_state=3407)

l1_word2vec.fit(trainvector_word2vec, y_train_encoded)

l1_pred_word2vec = l1_word2vec.predict(validvector_word2vec)
acc_l1_w2v = accuracy_score(y_valid_encoded, l1_pred_word2vec)
print("Accuracy_Score_w2v_l1", acc_l1_w2v)

"""#### 4.1.1.3 Word2Vec L2"""

# Create a logistic regression model with L2 regularization
# Cs are the inverse of regularization strengths
l2_word2vec = LogisticRegressionCV(Cs=1/alphas, cv=5, penalty='l2', solver='lbfgs', multi_class='multinomial', random_state=3407)

l2_word2vec.fit(trainvector_word2vec, y_train_encoded)

l2_pred_word2vec = l2_word2vec.predict(validvector_word2vec)
acc_l2_w2v = accuracy_score(y_valid_encoded, l2_pred_word2vec)
print("Accuracy_Score_w2v_l2", acc_l2_w2v)

"""### 4.1.2 BOW logit

#### 4.1.2.1 BOW No penalty
"""

logit_bow = LogisticRegression(penalty= None, multi_class='multinomial', solver='lbfgs', random_state=3407)
logit_bow.fit(x_bow_train, y_train_encoded)

logit_pred_bow = logit_bow.predict(x_bow_valid)
acc_logit_bow = accuracy_score(y_valid_encoded, logit_pred_bow)
print("Accuracy_Score_bow_logit", acc_logit_bow)

"""#### 4.1.2.2 BOW L1"""

l1_bow = LogisticRegressionCV(Cs=1/alphas, cv=5, penalty='l1', solver='liblinear', multi_class='ovr', random_state=3407)
l1_bow.fit(x_bow_train, y_train_encoded)

l1_pred_bow = l1_bow.predict(x_bow_valid)
acc_l1_bow = accuracy_score(y_valid_encoded, l1_pred_bow)
print("Accuracy_Score_bow_l1", acc_l1_bow)

"""#### 4.1.2.3 BOW L2"""

l2_bow = LogisticRegressionCV(Cs=1/alphas, cv=5, penalty='l2', solver='lbfgs', multi_class='multinomial', random_state=3407)
l2_bow.fit(x_bow_train, y_train_encoded)

l2_pred_bow = l2_bow.predict(x_bow_valid)
acc_l2_bow = accuracy_score(y_valid_encoded, l2_pred_bow)
print("Accuracy_Score_bow_l2", acc_l2_bow)

"""### 4.1.3 2-grams logit

#### 4.1.3.1 2-grams No penalty
"""

logit_bigram = LogisticRegression(penalty= None, multi_class='multinomial', solver='lbfgs', random_state=3407)
logit_bigram.fit(x_bigram_train, y_train_encoded)

logit_pred_bigram = logit_bigram.predict(x_bigram_valid)
acc_logit_bigram = accuracy_score(y_valid_encoded, logit_pred_bigram)
print("Accuracy_Score_bigram_logit", acc_logit_bigram)

"""#### 4.1.3.2 2-grams L1"""

l1_bigram = LogisticRegressionCV(Cs=1/alphas, cv=5, penalty='l1', solver='liblinear', multi_class='ovr', random_state=3407)
l1_bigram.fit(x_bigram_train, y_train_encoded)

l1_pred_bigram = l1_bigram.predict(x_bigram_valid)
acc_l1_bigram = accuracy_score(y_valid_encoded, l1_pred_bigram)
print("Accuracy_Score_bigram_l1", acc_l1_bigram)

"""#### 4.1.3.3 2-grams L2"""

l2_bigram = LogisticRegressionCV(Cs=1/alphas, cv=5, penalty='l2', solver='lbfgs', multi_class='multinomial', random_state=3407)
l2_bigram.fit(x_bigram_train, y_train_encoded)

l2_pred_bigram = l2_bigram.predict(x_bigram_valid)
acc_l2_bigram = accuracy_score(y_valid_encoded, l2_pred_bigram)
print("Accuracy_Score_bigram_l2", acc_l2_bigram)

"""### 4.1.4 3-grams logit

#### 4.1.4.1 3-grams no penalty
"""

logit_trigram = LogisticRegression(penalty= None, multi_class='multinomial', solver='lbfgs', random_state=3407)
logit_trigram.fit(x_trigram_train, y_train_encoded)

logit_pred_trigram = logit_trigram.predict(x_trigram_valid)
acc_logit_trigram = accuracy_score(y_valid_encoded, logit_pred_trigram)
print("Accuracy_Score_trigram_logit", acc_logit_trigram)

"""#### 4.1.4.2 3-grams L1"""

l1_trigram = LogisticRegressionCV(Cs=1/alphas, cv=5, penalty='l1', solver='liblinear', multi_class='ovr', random_state=3407)
l1_trigram.fit(x_trigram_train, y_train_encoded)

l1_pred_trigram = l1_trigram.predict(x_trigram_valid)

acc_l1_trigram = accuracy_score(y_valid_encoded, l1_pred_trigram)
print("Accuracy_Score_trigram_l1", acc_l1_trigram)

"""#### 4.1.4.3 3-grams L2"""

l2_trigram = LogisticRegressionCV(Cs=1/alphas, cv=5, penalty='l2', solver='lbfgs', multi_class='multinomial', random_state=3407)
l2_trigram.fit(x_trigram_train, y_train_encoded)

l2_pred_trigram = l2_trigram.predict(x_trigram_valid)

acc_l2_trigram = accuracy_score(y_valid_encoded, l2_pred_trigram)
print("Accuracy_Score_trigram_l2", acc_l2_trigram)

"""### 4.1.5 TF-IDF logit

#### 4.1.5.1 TF-IDF No Penalty
"""

logit_tfidf = LogisticRegression(penalty= None, multi_class='multinomial', solver='lbfgs', random_state=3407)
logit_tfidf.fit(x_tfidf_train, y_train_encoded)

logit_pred_tfidf = logit_tfidf.predict(x_tfidf_valid)

acc_logit_tfidf = accuracy_score(y_valid_encoded, logit_pred_tfidf)
print("Accuracy_Score_TFIDF_logit", acc_logit_tfidf)

"""#### 4.1.5.2 TF-IDF L1"""

l1_tfidf = LogisticRegressionCV(cv=5, penalty='l1', solver='liblinear', multi_class='ovr', random_state=3407)
l1_tfidf.fit(x_tfidf_train, y_train_encoded)

l1_pred_tfidf = l1_tfidf.predict(x_tfidf_valid)

acc_l1_tfidf = accuracy_score(y_valid_encoded, l1_pred_tfidf)
print("Accuracy_Score_TFIDF_L1", acc_l1_tfidf)

"""#### 4.1.5.3 TF-IDF L2"""

l2_tfidf = LogisticRegressionCV(cv=5, penalty='l2', solver='lbfgs', multi_class='multinomial', random_state=3407)
l2_tfidf.fit(x_tfidf_train, y_train_encoded)

l2_pred_tfidf = l2_tfidf.predict(x_tfidf_valid)

acc_l2_tfidf = accuracy_score(y_valid_encoded, l2_pred_tfidf)
print("Accuracy_Score_TFIDF_L2", acc_l2_tfidf)

"""### 4.1.6 Text Presentation selection"""

Logit = {
    'Text presentation': ['Word Embedding', 'Bag of words', 'Bigraam', 'Trigram','TF-IDF'],
    'Accuracy Score using Logistic regression': [acc_logit_w2v, acc_logit_bow, acc_logit_bigram, acc_logit_trigram, acc_logit_tfidf]
}
Logit_l1 = {'Text presentation': ['Word Embedding', 'Bag of words', 'Bigraam', 'Trigram','TF-IDF'],
    'Accuracy Score using Logistic regression L1': [acc_l1_w2v, acc_l1_bow, acc_l1_bigram, acc_l1_trigram, acc_l1_tfidf]
}
Logit_l2 = {'Text presentation': ['Word Embedding', 'Bag of words', 'Bigraam', 'Trigram','TF-IDF'],
    'Accuracy Score using Logistic regression L2': [acc_l2_w2v, acc_l2_bow, acc_l2_bigram, acc_l2_trigram, acc_l2_tfidf]
}

df = pd.concat([pd.DataFrame(Logit),pd.DataFrame(Logit_l1),pd.DataFrame(Logit_l2)], axis=1)

df

print(classification_report(y_valid_encoded,logit_pred_tfidf, digits=3))

"""## 4.2 Gradient Boosting Decision Tree

### 4.2.1 XGBoost

#### Optuna Optimization
"""

# This is the optuna optimization.

# from sklearn.metrics import accuracy_score
# def objective(trial):
#     # Define the search space for hyperparameters
#     param = {
#         'objective': 'multi:softprob',  # Multi-class classification
#         'eval_metric': 'mlogloss',  # Use logloss/merror for multi-class classification
#         'eta': trial.suggest_float('eta', 0.2, 0.4),
#         'num_boost_round': 300,  # Fix the boosting round and use early stopping
#         'max_depth': trial.suggest_int('max_depth', 3, 10),
#         'subsample': trial.suggest_float('subsample', 0.5, 1.0),
#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
#         'gamma': trial.suggest_float('gamma', 0.0, 10.0),
#         'min_child_weight': trial.suggest_float('min_child_weight', 0.1, 10.0),
#         'lambda': trial.suggest_float('lambda', 0.1, 10.0),
#         'alpha': trial.suggest_float('alpha', 0.0, 10.0),
#         'num_class': 5,  # Number of classes
#     }

#     # Convert the data into DMatrix format
#     dtrain = xgb.DMatrix(tfidf_train, label=y_train_encoded)
#     dvalid = xgb.DMatrix(tfidf_valid, label=y_valid_encoded)

#     # Define the pruning callback for early stopping
#     pruning_callback = optuna.integration.XGBoostPruningCallback(trial, 'validation-mlogloss')

#     # Train the model with early stopping
#     evals = [(dtrain, 'train'), (dvalid, 'validation')]
#     model = xgb.train(param, dtrain, evals=evals, num_boost_round=300, early_stopping_rounds=50, callbacks=[pruning_callback])

#     # Make predictions on the validation set
#     y_pred_proba = model.predict(dvalid)
#     y_pred = np.argmax(y_pred_proba, axis=1)  # Convert probabilities to class labels

#     # Calculate accuracy and logloss
#     accuracy = accuracy_score(y_valid_encoded, y_pred)
#     logloss = log_loss(y_valid_encoded, y_pred_proba)

#     # Return or print the losses and accuracy
#     print(f"Train Loss: {model.eval(dtrain)}")
#     print(f"Validation Loss: {model.eval(dvalid)}")
#     print(f"Validation Accuracy: {accuracy}")

#     return accuracy  # Or logloss if you want to minimize that

# # Create an Optuna study and optimize the objective function
# study = optuna.create_study(direction='maximize')  # Maximize accuracy for multi-class classification
# study.optimize(objective, n_trials=75)

# # Print the best hyperparameters and the best accuracy
# best_params = study.best_params
# best_accuracy = study.best_value
# print("Best Hyperparameters: ", best_params)
# print("Best Accuracy: ", best_accuracy)

# # history graph of optimization
# history_plot = vis.plot_optimization_history(study)
# history_plot.show()

# # Different importance weights from optimization
# importance_plot = vis.plot_param_importances(study)
# importance_plot.show()

"""#### Training XGBoost"""

# best_params.update({
#     'objective': 'multi:softprob',
#     'eval_metric': 'mlogloss',
#     'num_class': 5,
#     'seed': 3407
# })
# print(best_params)

dtrain = xgb.DMatrix(tfidf_train, label=y_train_encoded)
dvalid = xgb.DMatrix(tfidf_valid, label=y_valid_encoded)

loaded_model_xgb = checkpoint['XGB_model']
y_pred_proba = loaded_model_xgb.predict(dvalid)
y_pred = np.argmax(y_pred_proba, axis=1)

accuracy = accuracy_score(y_valid_encoded, y_pred)
print(f"Test Accuracy: {accuracy:.4f}")

# epochs = len(evals_result['train']['mlogloss'])
# x_axis = range(0, epochs)

# # Drawing a line chart to display the train and validation loss
# fig, ax = plt.subplots()
# ax.plot(x_axis, evals_result['train']['mlogloss'], label='Train Log Loss')
# ax.plot(x_axis, evals_result['eval']['mlogloss'], label='Validation Log Loss')
# ax.legend()
# plt.xlabel('Epochs')
# plt.ylabel('Log Loss')
# plt.title('XGBoost Log Loss across Epochs')
# plt.show()

print(classification_report(y_valid_encoded, y_pred, digits=3))

"""# 5.0 Model building (Deep learning)"""

torch.set_rng_state(checkpoint['rng_state'])
np.random.set_state(checkpoint['numpy_rng_state'])
random.setstate(checkpoint['random_state'])

"""## 5.1 Dataloader"""

train_for_nn = sub_train.copy()
val_for_nn = sub_valid.copy()
test_for_nn = test.copy()

# Because the data in content is in token, we combine the token into list
train_for_nn['category'] = train_for_nn['category'].apply(lambda x: ', '.join(x))
val_for_nn['category'] = val_for_nn['category'].apply(lambda x: ', '.join(x))
test_for_nn['category'] = test_for_nn['category'].apply(lambda x: ', '.join(x))

le = LabelEncoder()
train_target = le.fit_transform(train_for_nn['category']) # convert five categories into integers
val_target = le.transform(val_for_nn['category'])
test_target = le.transform(test_for_nn['category'])

train_for_nn['category'] = train_target
val_for_nn['category'] = val_target
test_for_nn['category'] = test_target

print(le.classes_) # this shows which index maps to which class

train_data = train_for_nn[['category','content']]
val_data = val_for_nn[['category','content']]
test_data = test_for_nn[['category','content']]

class TextDataset(utils.data.Dataset):
    def __init__(self, myData):
        """
        myData should be a dataframe object containing both y (first col) and X (second col)
        """
        super().__init__()
        self.data = myData

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return (self.data.iloc[idx,0], self.data.iloc[idx,1])

# build torch datasets
train_torch = TextDataset(train_data)
val_torch = TextDataset(val_data)
test_torch = TextDataset(test_data)

tokenizer = get_tokenizer('basic_english')

from torchtext.vocab import build_vocab_from_iterator

# ===== Build vocabulary =====
# an unknown token is added for all unknown words outside the documents
# you may specify the min_freq to filter out infrequent words
vocabulary = build_vocab_from_iterator(
    [tokenizer(msg) for msg in train_for_nn['content']],
    specials=["<unk>"],
    min_freq = 3, # filter out all words that appear less than three times
)
# Set to avoid errors with unknown words
vocabulary.set_default_index(vocabulary["<unk>"])

# define a function that converts a document into tokens (represented by index)
def doc_tokenizer(doc):
    return torch.tensor([vocabulary[token] for token in tokenizer(doc)], dtype=torch.long)

def collate_batch_advanced(batch):

    target_list, text_list = [], []

    # loop through all samples in batch
    for idx in range(len(batch)):

        _label = batch[idx][0]
        _text = batch[idx][1]

        target_list.append( _label )
        tokens = doc_tokenizer( _text )
        text_list.append(tokens)

    # convert to torch tensor
    target_list = torch.tensor(target_list, dtype=torch.int64)

    return target_list, text_list


# define the evaluate function, notice we need to pad each document with 0
def evaluate_adv(dataloader, model):
    y_pred = torch.tensor([], dtype=torch.float32)
    y_true = torch.tensor([], dtype=torch.float32)
    model.eval()
    with torch.no_grad():
        for label, text in dataloader:
            y_pred_batch = model(text)
            if y_pred_batch.dim() == 1:
                y_pred_batch = y_pred_batch.unsqueeze(0)
            y_pred = torch.cat((y_pred, y_pred_batch), dim=0)
            label = label.view(-1)
            y_true = torch.cat((y_true, label), dim=0)
    return y_pred, y_true

batchSize = 16
train_loader = utils.data.DataLoader(train_torch, batch_size=batchSize, shuffle=True, collate_fn=collate_batch_advanced)
val_loader = utils.data.DataLoader(val_torch, batch_size=batchSize, shuffle=False, collate_fn=collate_batch_advanced)
test_loader = utils.data.DataLoader(test_torch, batch_size=batchSize, shuffle=False, collate_fn=collate_batch_advanced)

class EarlyStopping:
    def __init__(self, patience=10, verbose=False, min_delta=0.0003):
        """
        Args:
            patience (int): The number of epochs allowed when validation loss is not improving.
            verbose (bool): If True，print the information.
            min_delta (float): the minimum threhold in improving the validation loss, if less than the threshold identify as no improvement
        """
        self.patience = patience
        self.verbose = verbose
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False

    def __call__(self, val_loss, model):
        if self.best_loss is None or val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
        else:
            self.counter += 1  # If no improvement, add 1 to counter
            if self.counter >= self.patience:
                self.early_stop = True  # early stopping
                if self.verbose:
                    print("Early stopping triggered")

def train_and_validate(model, train_loader, valid_loader, learning_rate, weight_decay):
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)
    valid_losses = []
    valid_accuracies = []
    train_losses = []
    early_stopper = EarlyStopping(patience=10, verbose=True)

    for epoch in range(10):
        model.train()
        train_loss = 0.0  # Initialize train loss for each epoch
        total_samples = 0
        for label, text in train_loader:
            optimizer.zero_grad()
            text = pad_sequence(text, batch_first=True, padding_value=0)

            outputs = model(text)
            loss = criterion(outputs, label)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * label.size(0)  # Multiply loss by batch size
            total_samples += label.size(0)

        average_train_loss = train_loss / total_samples
        train_losses.append(average_train_loss)  # Append average loss for the epoch

        y_pred_val, y_val = evaluate_adv(valid_loader, model)
        loss_val = criterion(y_pred_val, y_val.long())

        _, pred_labels = torch.max(y_pred_val, 1)
        valid_accuracy = (pred_labels == y_val).float().mean().item() * 100

        valid_losses.append(loss_val.item())
        valid_accuracies.append(valid_accuracy)

        print(f'Epoch [{epoch+1}/{100}], Train Loss: {train_losses[-1]:.4f}, Valid Loss: {valid_losses[-1]:.4f}， Valid Accuracy: {valid_accuracies[-1]:.2f}%')

        early_stopper(loss_val / len(valid_loader), model)
        if early_stopper.early_stop:
            print(f"Early stopping at epoch {epoch + 1}")
            break
    return max(valid_accuracies), train_losses, valid_losses

def predict(model, data_loader):
    model.eval()
    all_preds = []
    all_probs = []
    all_labels = []
    with torch.no_grad():
        for labels, texts in data_loader:
            outputs = model(texts)
            probabilities = F.softmax(outputs, dim=1)
            predicted_classes = torch.argmax(probabilities, dim=1)

            all_preds.extend(predicted_classes.numpy())
            all_probs.extend(probabilities.numpy())
            all_labels.extend(labels.numpy().flatten())

    return np.array(all_preds), np.array(all_probs), np.array(all_labels)

"""## 5.2 Feedforward prepare data"""

vocab_size = len(vocabulary)

class FeedforwardNN(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout_rate):
        super(FeedforwardNN, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.fc1 = nn.Linear(embedding_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x):
        x = pad_sequence(x, batch_first=True, padding_value=0)
        embedded = self.embedding(x)
        embedded = embedded.mean(dim=1)
        out = self.fc1(embedded)
        out = torch.relu(out)
        out = self.dropout(out)
        out = self.fc2(out)
        return out

def evaluate_model(model, val_loader, criterion):
    model.eval()
    total_loss, correct, total = 0, 0, 0
    with torch.no_grad():
        for label, text in val_loader:
            output = model(text)
            loss = criterion(output, label)
            total_loss += loss.item()

            _, predicted = torch.max(output, 1)
            correct += (predicted == label).sum().item()
            total += label.size(0)

    avg_loss = total_loss / len(val_loader)
    accuracy = correct / total
    return avg_loss, accuracy

"""###5.2.1 FNN optuna

"""

# def objective(trial):

#     embedding_dim = trial.suggest_int('embedding_dim', 16, 32)
#     hidden_dim = trial.suggest_int('hidden_dim', 16, 30)
#     dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)
#     learning_rate = trial.suggest_loguniform('lr', 1e-4, 1e-2)
#     patience = 10

#     model = FeedforwardNN(len(vocabulary), embedding_dim, hidden_dim, len(le.classes_), dropout_rate)

#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)
#     criterion = nn.CrossEntropyLoss()

#     best_val_loss = float('inf')
#     best_epoch = 0
#     num_epochs = 30
#     early_stop = False

#     for epoch in range(num_epochs):
#         if early_stop:
#             break

#         model.train()
#         total_train_loss = 0
#         for label, text in train_loader:
#             optimizer.zero_grad()
#             output = model(text)
#             loss = criterion(output, label)
#             loss.backward()
#             optimizer.step()
#             total_train_loss += loss.item()

#         avg_train_loss = total_train_loss / len(train_loader)


#         val_loss, val_accuracy = evaluate_model(model, val_loader, criterion)

#         if val_loss < best_val_loss:
#             best_val_loss = val_loss
#             best_epoch = epoch
#         elif epoch - best_epoch >= patience:
#             early_stop = True

#         print(f"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_train_loss:.4f} | Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f}")

#         # Prune trial if no improvement
#         trial.report(val_loss, epoch)

#         if trial.should_prune():
#             raise optuna.exceptions.TrialPruned()

#     return best_val_loss

# study = optuna.create_study(direction='minimize')
# study.optimize(objective, n_trials=30)

# print("Best trial:")
# trial = study.best_trial
# print("  Value: {}".format(trial.value))
# print("  Params: ")
# for key, value in trial.params.items():
#     print("    {}: {}".format(key, value))

"""###5.2.2 Load FNN best parameter"""

FNN_best_params = checkpoint['FNN_best_params']

FNN =  FeedforwardNN(
    vocab_size=len(vocabulary),
    embedding_dim=FNN_best_params['embedding_dim'],
    hidden_dim=FNN_best_params['hidden_dim'],
    output_dim=len(le.classes_),
    dropout_rate=FNN_best_params['dropout_rate']

)
train_loader = DataLoader(train_torch, batch_size=FNN_best_params['BATCH_SIZE'], shuffle=True, collate_fn=collate_batch_advanced)
val_loader = DataLoader(val_torch, batch_size=FNN_best_params['BATCH_SIZE'], shuffle=False, collate_fn=collate_batch_advanced)

"""###5.2.3 Train FNN"""

# class TextDataset(torch.utils.data.Dataset):
#     def __init__(self, myData):
#         super().__init__()
#         self.data = myData

#     def __len__(self):
#         return len(self.data)

#     def __getitem__(self, idx):
#         return (self.data.iloc[idx, 0], self.data.iloc[idx, 1])


# tokenizer = get_tokenizer('basic_english')
# vocabulary = build_vocab_from_iterator(
#     [tokenizer(msg) for msg in train_for_nn['content']],
#     specials=["<unk>"],
#     min_freq=3,
# )
# vocabulary.set_default_index(vocabulary["<unk>"])

# def doc_tokenizer(doc):
#     return torch.tensor([vocabulary[token] for token in tokenizer(doc)], dtype=torch.long)

# def collate_batch(batch):
#     target_list, text_list = [], []
#     for label, text in batch:
#         target_list.append(label)

#         if isinstance(text, list):
#             text = ' '.join(text)
#         tokens = doc_tokenizer(text)
#         text_list.append(tokens)
#     target_list = torch.tensor(target_list, dtype=torch.int64)
#     text_list = pad_sequence(text_list, batch_first=True, padding_value=0)
#     return target_list, text_list


# batch_size = 16
# train_loader = DataLoader(TextDataset(train_data), batch_size=batch_size, shuffle=True, collate_fn=collate_batch)
# val_loader = DataLoader(TextDataset(val_data), batch_size=batch_size, shuffle=False, collate_fn=collate_batch)

def train_and_validate_FNN(model, train_loader, valid_loader, learning_rate):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    valid_accuracies = []
    train_losses = []
    valid_losses = []
    valid_f1_scores = []

    device = torch.device('cpu')

    for epoch in range(50):
        model.train()
        train_loss = 0.0
        total_samples = 0

        for batch in train_loader:
            optimizer.zero_grad()

            labels, input_data = batch
            labels = labels.to(device)
            input_data = input_data.to(device)

            outputs = model(input_data)

            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * labels.size(0)
            total_samples += labels.size(0)

        average_train_loss = train_loss / total_samples
        train_losses.append(average_train_loss)


        model.eval()
        y_pred_val = []
        y_val = []
        valid_loss = 0.0
        total_valid_samples = 0

        with torch.no_grad():
            for batch in valid_loader:
                labels, input_data = batch
                labels = labels.to(device)
                input_data = input_data.to(device)

                outputs = model(input_data)


                loss = criterion(outputs, labels)
                valid_loss += loss.item() * labels.size(0)
                total_valid_samples += labels.size(0)

                _, preds = torch.max(outputs, 1)
                y_pred_val.extend(preds.cpu().numpy())
                y_val.extend(labels.cpu().numpy())


        average_valid_loss = valid_loss / total_valid_samples
        valid_losses.append(average_valid_loss)

        val_accuracy = (np.array(y_pred_val) == np.array(y_val)).mean() * 100
        valid_accuracies.append(val_accuracy)


        f1 = f1_score(y_val, y_pred_val, average='weighted')
        valid_f1_scores.append(f1)

        print(f'Epoch [{epoch+1}/50], Train Loss: {train_losses[-1]:.4f}, Valid Loss: {valid_losses[-1]:.4f}, Valid Accuracy: {valid_accuracies[-1]:.2f}%, F1 Score: {f1:.4f}')


    return train_losses, valid_losses, valid_accuracies, valid_f1_scores

# This is the train and validation loss. Since we use the loaded model directly, this code is commented out.

# train_losses, valid_losses, valid_accuracies, valid_f1_scores = train_and_validate_FNN(
#     FNN, train_loader, val_loader,
#     learning_rate=FNN_best_params['lr'])

"""###5.2.4 Load thje final trained FNN Model and parameter"""

FNN.load_state_dict(checkpoint['FNN_state_dict'])

y_pred, y_probs, all_labels = predict(FNN, val_loader)

accuracy = accuracy_score(all_labels, y_pred)
print(f"Test Accuracy: {accuracy:.4f}")

# This is the train and validation loss. Since we use the loaded model directly, this code is commented out.

# x_axis = range(0, len(train_losses))
# fig, ax = plt.subplots()
# ax.plot(x_axis, train_losses, label='Training Loss')
# ax.plot(x_axis, valid_losses, label='Validation Loss')
# ax.set_xlabel('Epochs')
# ax.set_ylabel('Loss')
# ax.set_title('RNN Train and Validation Loss per Epoch')
# ax.legend()
# plt.show()

print(classification_report(all_labels, y_pred, digits=3))

"""## 5.3 Vanila Recurrent

"""

class RNNClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_RNNLayer, dropout):
        super(RNNClassifier, self).__init__()

        self.hidden_dim = hidden_dim
        self.num_RNNLayer = num_RNNLayer
        # Embedding layer
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        # Use layer normalization to normalize
        self.embed_norm = nn.LayerNorm(embedding_dim)
        self.rnn = nn.RNN(input_size=embedding_dim,
                          hidden_size=hidden_dim,
                          num_layers= num_RNNLayer,
                          batch_first=True)
        # Dropout
        self.dropout = nn.Dropout(dropout)
        self.linear = nn.Linear(hidden_dim, 5)

    def forward(self, x):
        x = pad_sequence(x, batch_first=True, padding_value=0)
        h = torch.zeros(self.num_RNNLayer, x.size(0), self.hidden_dim)
        torch.nn.init.xavier_normal_(h)

        out = self.embedding(x)
        out = self.embed_norm(out)
        out, h = self.rnn(out, h)
        # Average pooling
        out = torch.mean(out, dim=1)
        out = self.dropout(out)
        out = self.linear(out)
        return out

def RNN_train_and_validate(model, train_loader, valid_loader, learning_rate, NUM_EPOCHS, weight_decay):
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)
    valid_losses = []
    valid_accuracies = []
    train_losses = []
    early_stopper = EarlyStopping(patience=10, verbose=True)

    for epoch in range(NUM_EPOCHS): # loop through epochs
        model.train()
        train_loss = 0.0  # Initialize train loss for each epoch
        total_samples = 0
        for label, text in train_loader:
            optimizer.zero_grad()
            text = pad_sequence(text, batch_first=True, padding_value=0)

            outputs = model(text)
            loss = criterion(outputs, label)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * label.size(0)  # Multiply loss by batch size
            total_samples += label.size(0)

        average_train_loss = train_loss / total_samples
        train_losses.append(average_train_loss)  # Append average loss for the epoch

        y_pred_val, y_val = evaluate_adv(valid_loader, model)
        loss_val = criterion(y_pred_val, y_val.long())

        _, pred_labels = torch.max(y_pred_val, 1)
        valid_accuracy = (pred_labels == y_val).float().mean().item() * 100

        valid_losses.append(loss_val.item())
        valid_accuracies.append(valid_accuracy)

        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {train_losses[-1]:.4f}, Valid Loss: {valid_losses[-1]:.4f}， Valid Accuracy: {valid_accuracies[-1]:.2f}%')

        early_stopper(loss_val / len(valid_loader), model)
        if early_stopper.early_stop:
            print(f"Early stopping at epoch {epoch + 1}")
            break
    return max(valid_accuracies), train_losses, valid_losses

# This is the optuna optimization.

# def objective(trial):
#     vocab_size = len(vocabulary)
#     embedding_dim = trial.suggest_int('embedding_dim', 64, 128)
#     hidden_dim = trial.suggest_int('hidden_dim', 32, 96)
#     num_RNNLayer = trial.suggest_int('num_RNNLayer', 1, 2)
#     dropout = trial.suggest_float('dropout', 0.35, 0.7)
#     learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2)
#     weight_decay = trial.suggest_float('weight_decay', 1e-4, 1e-2)
#     BATCH_SIZE = 32,
#     NUM_EPOCHS = 50

#     train_loader = DataLoader(train_torch, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch_advanced)
#     val_loader = DataLoader(val_torch, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch_advanced)

#     model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, num_RNNLayer, dropout)
#     valid_accuracies, _, _= RNN_train_and_validate(model, train_loader, val_loader, learning_rate, NUM_EPOCHS, weight_decay)
#     return valid_accuracies

# study = optuna.create_study(direction='maximize')
# study.optimize(objective, n_trials=50)

# best_params = study.best_params
# best_accuracy = study.best_value
# print("Best Hyperparameters: ", best_params)
# print("Best accuracy: ", best_accuracy)

# history_plot = vis.plot_optimization_history(study)
# history_plot.show()

# importance_plot = vis.plot_param_importances(study)
# importance_plot.show()

RNN_best_params = checkpoint['RNN_best_params_state_dict']

RNN_model = RNNClassifier(
    vocab_size=RNN_best_params['vocab_size'],
    embedding_dim=RNN_best_params['embedding_dim'],
    hidden_dim=RNN_best_params['hidden_dim'],
    num_RNNLayer=RNN_best_params['num_RNNLayer'],
    dropout=RNN_best_params['dropout']
)

def RNN_train_and_validate_update(model, train_loader, valid_loader, learning_rate, NUM_EPOCHS, weight_decay, model_save_path):
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)
    valid_losses = []
    valid_accuracies = []
    train_losses = []

    early_stopper = EarlyStopping(patience=50, verbose=True)

    best_accuracy = 0.0
    best_epoch = 0

    for epoch in range(NUM_EPOCHS):
        model.train()
        train_loss = 0.0
        total_samples = 0
        for label, text in train_loader:
            optimizer.zero_grad()
            text = pad_sequence(text, batch_first=True, padding_value=0)

            outputs = model(text)
            loss = criterion(outputs, label)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * label.size(0)
            total_samples += label.size(0)

        average_train_loss = train_loss / total_samples
        train_losses.append(average_train_loss)

        y_pred_val, y_val = evaluate_adv(valid_loader, model)
        loss_val = criterion(y_pred_val, y_val.long())

        _, pred_labels = torch.max(y_pred_val, 1)
        valid_accuracy = (pred_labels == y_val).float().mean().item() * 100

        valid_losses.append(loss_val.item())
        valid_accuracies.append(valid_accuracy)

        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {train_losses[-1]:.4f}, Valid Loss: {valid_losses[-1]:.4f}， Valid Accuracy: {valid_accuracies[-1]:.2f}%')
        # Save the best model and calculate the best accuracy
        if valid_accuracy > best_accuracy:
            best_accuracy = valid_accuracy
            best_epoch = epoch + 1
            torch.save(model.state_dict(), model_save_path)

        early_stopper(loss_val / len(valid_loader), model)
        if early_stopper.early_stop:
            print(f"Early stopping at epoch {epoch + 1}")
            break
    print(f'Best Validation Accuracy: {best_accuracy:.2f}% at epoch {best_epoch}')

    return best_accuracy, train_losses, valid_losses

# This is the train and validation loss. Since we use the loaded model directly, this code is commented out.

# RNN_train_loader = DataLoader(train_torch, batch_size=RNN_best_params['BATCH_SIZE'], shuffle=True, collate_fn=collate_batch_advanced)
# RNN_val_loader = DataLoader(val_torch, batch_size=RNN_best_params['BATCH_SIZE'], shuffle=False, collate_fn=collate_batch_advanced)
# _, train_losses, valid_losses = RNN_train_and_validate_update(RNN_model, RNN_train_loader, RNN_val_loader, RNN_best_params['learning_rate'], RNN_best_params['NUM_EPOCHS'], RNN_best_params['weight_decay'], model_save_path='/content/drive/MyDrive/6850/final_rnn_model_state_dict.pth')

RNN_model.load_state_dict(checkpoint['RNN_model_state_dict'])

y_pred, y_probs, all_labels = predict(RNN_model, val_loader)

accuracy = accuracy_score(all_labels, y_pred)
print(f"Test Accuracy: {accuracy:.4f}")

# This is the train and validation loss. Since we use the loaded model directly, this code is commented out.

# x_axis = range(0, len(train_losses))
# fig, ax = plt.subplots()
# ax.plot(x_axis, train_losses, label='Training Loss')
# ax.plot(x_axis, valid_losses, label='Validation Loss')
# ax.set_xlabel('Epochs')
# ax.set_ylabel('Loss')
# ax.set_title('RNN Train and Validation Loss per Epoch')
# ax.legend()
# plt.show()

print(classification_report(all_labels, y_pred, digits=3))

"""## 5.4 LSTM

### 5.4.1 Optimization with Optuna
"""

import torch
import torch.nn as nn
from torch.nn.utils.rnn import pad_sequence

class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_LstmLayer, dropout_rate, fc1_dim):
        super().__init__()

        self.hidden_dim = hidden_dim
        self.num_LstmLayer = num_LstmLayer

        # 1. Embedding layer
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        # 2. Layer Normalizaion after embedding
        self.embed_norm = nn.LayerNorm(embed_dim)
        # 3. LSTM layer
        self.lstm = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim, num_layers=num_LstmLayer, batch_first=True)
        # 4. Dropout layer (moved after LSTM)
        self.dropout = nn.Dropout(p=dropout_rate)
        # 5. First fully connected layer
        self.Linear1 = nn.Linear(hidden_dim, fc1_dim)
        # 6. Second fully connected layer
        self.Linear2 = nn.Linear(fc1_dim, 5)

        # For residual connection in the fully connected layers
        if hidden_dim != fc1_dim:
            self.residual_fc = nn.Linear(hidden_dim, fc1_dim)
        else:
            self.residual_fc = None

    def forward(self, x):
        x = pad_sequence(x, batch_first=True, padding_value=0)

        # Initialize hidden state and cell state
        h = torch.zeros((self.num_LstmLayer, x.size(0), self.hidden_dim)).to(x.device)
        c = torch.zeros((self.num_LstmLayer, x.size(0), self.hidden_dim)).to(x.device)

        # 1. Embedding layer
        out = self.embedding(x)
        # 2. Nrom after Embedding
        out = self.embed_norm(out)
        # 3. LSTM layer
        out, (hidden, cell) = self.lstm(out, (h, c))
        # 4. Layer Normalization and Dropout
        out, _ = torch.max(out, dim=1)
        out = self.dropout(out)

        # Residual connection starts here
        # 5. First fully connected layer with possible residual
        residual = out  # Save for the residual connection
        out = self.Linear1(out)
        out = torch.relu(out)

        if self.residual_fc is not None:
            residual = self.residual_fc(residual)  # Make dimensions match if needed
        out = out + residual  # Add the residual connection

        # 6. Second fully connected layer
        out = self.Linear2(out)

        return out

#This is the optuna optimization.

# def objective(trial):
#     embed_dim = trial.suggest_int('embed_dim', 86, 158)
#     hidden_dim = trial.suggest_int('hidden_dim', 64, 158)
#     dropout_rate = trial.suggest_float('dropout_rate', 0.45, 0.65)
#     learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)
#     weight_decay = trial.suggest_float('weight_decay', 1e-4, 1e-2)
#     fc1_dim = trial.suggest_int('fc1_dim', 86, 158)
#     num_LstmLayer = trial.suggest_int('num_LstmLayer', 1, 2)

#     # initialize model
#     model = LSTMClassifier(len(vocabulary), embed_dim, hidden_dim, num_LstmLayer, dropout_rate, fc1_dim)

#     valid_accuracies, _, _= train_and_validate(model, train_loader, val_loader, learning_rate, weight_decay)

#     return valid_accuracies

# study = optuna.create_study(direction='maximize') # maximize the accuracy
# study.optimize(objective, n_trials=100)

# best_params = study.best_params
# best_accuracy = study.best_value
# print("Best Hyperparameters: ", best_params)
# print("Best accuracy: ", best_accuracy)

"""### 5.4.2 Optimization visualization"""

# history_plot = vis.plot_optimization_history(study)
# history_plot.show()

# importance_plot = vis.plot_param_importances(study)
# importance_plot.show()

"""### 5.4.3 Load pre_trained best parameters and model"""

LSTM_best_params = checkpoint['LSTM_best_params_state_dict']

LSTM_before_concat = LSTMClassifier(
    vocab_size=LSTM_best_params['vocab_size'],
    embed_dim=LSTM_best_params['embed_dim'],
    hidden_dim=LSTM_best_params['hidden_dim'],
    num_LstmLayer=LSTM_best_params['num_LstmLayer'],
    dropout_rate=LSTM_best_params['dropout_rate'],
    fc1_dim = LSTM_best_params['fc1_dim']
)
train_loader = DataLoader(train_torch, batch_size=LSTM_best_params['BATCH_SIZE'], shuffle=True, collate_fn=collate_batch_advanced)
val_loader = DataLoader(val_torch, batch_size=LSTM_best_params['BATCH_SIZE'], shuffle=False, collate_fn=collate_batch_advanced)

"""### 5.4.4 Train the model"""

def train_and_validate(model, train_loader, valid_loader, learning_rate, weight_decay):
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)
    valid_losses = []
    valid_accuracies = []
    train_losses = []
    early_stopper = EarlyStopping(patience=10, verbose=True)

    for epoch in range(50):
        model.train()
        train_loss = 0.0  # Initialize train loss for each epoch
        total_samples = 0
        for label, text in train_loader:
            optimizer.zero_grad()
            text = pad_sequence(text, batch_first=True, padding_value=0)

            outputs = model(text)
            loss = criterion(outputs, label)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * label.size(0)  # Multiply loss by batch size
            total_samples += label.size(0)

        average_train_loss = train_loss / total_samples
        train_losses.append(average_train_loss)  # Append average loss for the epoch

        y_pred_val, y_val = evaluate_adv(valid_loader, model)
        loss_val = criterion(y_pred_val, y_val.long())

        _, pred_labels = torch.max(y_pred_val, 1)
        valid_accuracy = (pred_labels == y_val).float().mean().item() * 100

        valid_losses.append(loss_val.item())
        valid_accuracies.append(valid_accuracy)

        print(f'Epoch [{epoch+1}/{100}], Train Loss: {train_losses[-1]:.4f}, Valid Loss: {valid_losses[-1]:.4f}， Valid Accuracy: {valid_accuracies[-1]:.2f}%')

        early_stopper(loss_val / len(valid_loader), model)
        if early_stopper.early_stop:
            print(f"Early stopping at epoch {epoch + 1}")
            break
    return max(valid_accuracies), train_losses, valid_losses

# This is the train and validation loss. Since we use the loaded model directly, this code is commented out.

# _, train_losses, valid_losses = train_and_validate(
#     LSTM_before_concat, train_loader, val_loader,
#     LSTM_best_params['learning_rate'], LSTM_best_params['weight_decay'])

LSTM_before_concat.load_state_dict(checkpoint['LSTM_before_concat_state_dict'])

y_pred, y_probs, all_labels = predict(LSTM_before_concat, val_loader)

accuracy = accuracy_score(all_labels, y_pred)
print(f"Test Accuracy: {accuracy:.4f}")

# This is the train and validation loss. Since we use the loaded model directly, this code is commented out.

# x_axis = range(0, len(train_losses))
# fig, ax = plt.subplots()
# ax.plot(x_axis, train_losses, label='Training Loss')
# ax.plot(x_axis, valid_losses, label='Validation Loss')
# ax.set_xlabel('Epochs')
# ax.set_ylabel('Loss')
# ax.set_title('RNN Train and Validation Loss per Epoch')
# ax.legend()
# plt.show()

print(classification_report(all_labels, y_pred, digits=3))

"""# 6.0 Model selection and evaluation

## 6.1 Load pre_trained best model
"""

Train_data = pd.concat([train_data, val_data], axis=0)

Train_torch = TextDataset(Train_data)

batchSize = 16
Train_loader = utils.data.DataLoader(Train_torch, batch_size=batchSize, shuffle=True, collate_fn=collate_batch_advanced)

class EarlyStopping_for_MS:
    def __init__(self, acc_threshold=0.96, verbose=False):#set threshold to 96%

        self.acc_threshold = acc_threshold
        self.verbose = verbose
        self.early_stop = False

    def __call__(self, val_acc, model):
        if val_acc >= self.acc_threshold:
            self.early_stop = True
            if self.verbose:
                print(f"Early stopping triggered at validation accuracy {val_acc:.2f} (>= {self.acc_threshold:.2f})")

def train_and_validate_for_MS_1st(model, train_loader, valid_loader, learning_rate, weight_decay, model_save_path):
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)
    valid_losses = []
    valid_accuracies = []
    train_losses = []
    best_accuracy = 0.0
    best_epoch = 0

    for epoch in range(100):
        model.train()
        train_loss = 0.0  # Initialize train loss for each epoch
        total_samples = 0
        for label, text in train_loader:
            optimizer.zero_grad()
            text = pad_sequence(text, batch_first=True, padding_value=0)

            outputs = model(text)
            loss = criterion(outputs, label)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * label.size(0)  # Multiply loss by batch size
            total_samples += label.size(0)

        average_train_loss = train_loss / total_samples
        train_losses.append(average_train_loss)  # Append average loss for the epoch

        y_pred_val, y_val = evaluate_adv(valid_loader, model)
        loss_val = criterion(y_pred_val, y_val.long())

        _, pred_labels = torch.max(y_pred_val, 1)
        valid_accuracy = (pred_labels == y_val).float().mean().item() * 100

        valid_losses.append(loss_val.item())
        valid_accuracies.append(valid_accuracy)

        print(f'Epoch [{epoch+1}/{100}], Train Loss: {train_losses[-1]:.4f}, Valid Loss: {valid_losses[-1]:.4f}， Valid Accuracy: {valid_accuracies[-1]:.2f}%')

        if valid_accuracy > best_accuracy:
            best_accuracy = valid_accuracy
            best_epoch = epoch + 1  # Update best_epoch
            torch.save(model.state_dict(), model_save_path)

    print(f'Best Accuracy: {best_accuracy:.2f}% at epoch {best_epoch}')
    return best_accuracy, train_losses, valid_losses,

"""## 6.2 Train the final LSTM"""

LSTM_after_concat = LSTMClassifier(
    vocab_size=LSTM_best_params['vocab_size'],
    embed_dim=LSTM_best_params['embed_dim'],
    hidden_dim=LSTM_best_params['hidden_dim'],
    num_LstmLayer=LSTM_best_params['num_LstmLayer'],
    dropout_rate=LSTM_best_params['dropout_rate'],
    fc1_dim = LSTM_best_params['fc1_dim']
)

# This is the train and validation loss. Since we use the loaded model directly, this code is commented out.

# best_accuracy, train_losses, valid_losses = train_and_validate_for_MS_1st(LSTM_after_concat, Train_loader, test_loader, LSTM_best_params['learning_rate'], LSTM_best_params['weight_decay'],model_save_path='/content/drive/MyDrive/6850/LSTM_after_concat.pth')

"""## 6.3 Model evaluation"""

LSTM_after_concat.load_state_dict(checkpoint['LSTM_after_concat_state_dict'])

y_pred, y_probs, all_labels = predict(LSTM_after_concat, test_loader)

accuracy = accuracy_score(all_labels, y_pred)
print(f"Test Accuracy: {accuracy:.4f}")

print(classification_report(all_labels, y_pred, digits=3))

"""# 7.0 Predict"""

competition = pd.read_csv('/content/drive/MyDrive/6850/news-challenge.csv',sep='\t')

competition = competition[['content']]

def tokenizer(text):
    text = re.sub(r'#(\S+)', 'xxhashtag ' + r'\1', text) # hashtag

    text = re.sub(r'\s{2,}', ' ', text)

    doc = nlp(text)

    tokens = []

    for token in doc:

        word = token.lemma_.lower()

        if not token.is_stop:

            if word == '!':
                tokens.append('!')

            elif (not token.is_punct) and word != '️':
                tokens.append(word)

    return tokens

competition.iloc[:,0] = competition.iloc[:,0].apply(tokenizer)

competition['content'] = competition['content'].apply(lambda x: ' '.join(x))

class TextDataset_competition(utils.data.Dataset):
    def __init__(self, myData):
        """
        myData: a dataframe object containing only X (input data).
        """
        super().__init__()
        self.data = myData

    def __len__(self):
        return len(self.data)
    def __getitem__(self, idx):
        return self.data.iloc[idx]

competition_torch = TextDataset_competition(competition['content'])

tokenizer = get_tokenizer('basic_english')

vocabulary = build_vocab_from_iterator(
    [tokenizer(msg) for msg in train_for_nn['content']],
    specials=["<unk>"],
    min_freq = 3, # filter out all words that appear less than three times
)
# Set to avoid errors with unknown words
vocabulary.set_default_index(vocabulary["<unk>"])

def doc_tokenizer(doc):
    return torch.tensor([vocabulary[token] for token in tokenizer(doc)], dtype=torch.long)

def collate_batch_competition(batch):

    text_list = []

    # loop through all samples in batch
    for idx in range(len(batch)):

        _text = batch[idx]
        tokens = doc_tokenizer( _text )
        text_list.append(tokens)

    # convert to torch tensor

    return text_list

batchSize = 16
competition_loader = utils.data.DataLoader(competition_torch, batch_size=batchSize, shuffle=False, collate_fn=collate_batch_competition)

def predict_competition(model, data_loader):
    model.eval()  # evaluate
    all_preds = []  # save the classes
    all_probs = []  # save the probability
    with torch.no_grad():
        for texts in data_loader:
            outputs = model(texts)  # forward propagation
            probabilities = F.softmax(outputs, dim=1)  # calculate probabilities through softmax
            predicted_classes = torch.argmax(probabilities, dim=1)  # get the class(the max probabilities)

            all_preds.extend(predicted_classes.numpy())  # save the class into all_preds
            all_probs.extend(probabilities.numpy())  # save the probability into all_probs

    return np.array(all_preds), np.array(all_probs)  # return category and probability

predictions, probabilities = predict_competition(LSTM_after_concat, competition_loader)

predictions

original_categories = le.inverse_transform(predictions)

print(original_categories)

predictions_df = pd.DataFrame({
    'ID': range(len(original_categories)),  # Create an ID column, sorted in order
    'category': original_categories  # the predicted class (category)
})

predictions_df.head(30)

output_path = '/content/drive/MyDrive/6850/Group09QBUS6850_2024S2Comma.csv'
# save datafrme into csv file
predictions_df.to_csv(output_path, index=False)  # set index = False, to make sure index is not save into the csv file

"""# 8.0 Save all the models and hyperparameters"""

torch.save({
    'device': 'cpu',
    'LSTM_after_concat_state_dict': LSTM_after_concat.state_dict(),
    'LSTM_before_concat_state_dict': LSTM_before_concat.state_dict(),
    'LSTM_best_params_state_dict': LSTM_best_params,
    'RNN_best_params_state_dict': RNN_best_params,
    'RNN_model_state_dict': RNN_model.state_dict(),
    'FNN_state_dict': FNN.state_dict(),
    'FNN_best_params': FNN_best_params,
    'XGB_model': loaded_model_xgb,

    'rng_state': torch.get_rng_state(),
    'numpy_rng_state': np.random.get_state(),
    'random_state': random.getstate()
}, '/content/drive/MyDrive/6850/checkpoint_ML&DL.pth')

torch.save({
    'model_state_dict': LSTM_after_concat.state_dict(),
    'best_params': LSTM_best_params,
    'vocabulary': vocabulary
}, "/content/drive/MyDrive/6850/Group09QBUS6850_best_2024S2.pt")